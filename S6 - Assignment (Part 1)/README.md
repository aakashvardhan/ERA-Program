# Neural Network Backpropogation

## Introduction

- Backpropogation is the most fundamental building block of a neural network. It is used to calculate the gradient of a loss function with respect to the weights of the network, "computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule"[cited: https://en.wikipedia.org/wiki/Backpropagation]

![image](https://github.com/aakashvardhan/ERA-Program/blob/master/S6%20-%20Assignment%20(Part%201)/243159437-5388e964-b282-4f6e-a3de-2b0848001ecb.png)

## Tutorial

- $i_1$ and $i_2$